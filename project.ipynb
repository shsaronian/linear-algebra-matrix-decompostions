{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Mega Project 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Abstract</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project first focuses on the Spectral Decomposition or Eigen Decomposition of matrices using common python packages. The goal is to have a program which will take a matrix as an input, and output the Spectral Decomposition for it. The packages used will be numpy and scipy.linalg which have the eigen-decomposition process bulit in. Next, the output will be evaluated for special types of matrices, that is, for symmetric, positive definite, diagonal, block, non-singular, and non-square examples. \n",
    "\n",
    "In the second phase, we will implement the QR decomposition from scracth using three methods: the Grahm-Shmidt process, the Householder Reflections, and the Givens Transformations. Then, using either one of these, the QR algorithm is implemented and through that the eigenvalues and eigenvectors of a given square matrix can be calculated. We will then present a brief summary on which type of QR decomposition is suitable for which type of matrices, with arguments.\n",
    "\n",
    "The third phase is bulit completely on top of the second one, as we wish to calculate the SVD decomposition of a given non-square matrix. The program uses the QR algorithm from the previous phase, and in another implemented algorithm, calculates the singular values and vectors with the help of the QR algorithm. \n",
    "\n",
    "Lastly in the fourth phase we point at some applications of phases 2,3 in several aspects of Data Mining, with some square and non-square examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the necessary packages first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from math import copysign, hypot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few lines ask the user to enter the number of rows and columns, and then enter the matrix values one by one in a single line seperated by spaces. Line 5 takes these entries, converts them to float, splits them according to the space locations and stores the seperated values in a list named entries. Let's try a regular 3x3 square matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:3\n",
      "Enter the number of columns:3\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "1 2 3 4 5 6 7 8 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line convert the entries list to a corresponding matrix with dimensions specified by the user above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array(entries).reshape(n, m) \n",
    "print(\"A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now we have our matrix. Line 1 invokes the eig() method from scipy.linealg, which automatically computes the eigenvectors and values and stores them in λ,U. We print those first, then continue by making a diagonal matrix D out of eigenvalues, where the values are placed on the diagonal of D. We then calculate U inverse, and then print U, D and U inverse. That indicates the eigendecomposition of A, that is, the eigenvectors of A are columns of U, and the eigenvalues of A are values placed on the diagonal of D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ:\n",
      "[ 1.61168440e+01 -1.11684397e+00 -1.30367773e-15]\n",
      "U:\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n",
      "D:\n",
      "[[ 1.61168440e+01+0.j  0.00000000e+00+0.j  0.00000000e+00+0.j]\n",
      " [ 0.00000000e+00+0.j -1.11684397e+00+0.j  0.00000000e+00+0.j]\n",
      " [ 0.00000000e+00+0.j  0.00000000e+00+0.j -1.30367773e-15+0.j]]\n",
      "U inverse:\n",
      "[[-0.48295226 -0.59340999 -0.70386772]\n",
      " [-0.91788599 -0.24901003  0.41986593]\n",
      " [ 0.40824829 -0.81649658  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "λ,U = la.eig(A)\n",
    "print(\"λ:\")\n",
    "print(λ.real)\n",
    "print(\"U:\")\n",
    "print(U.real)\n",
    "print(\"D:\")\n",
    "print(np.diag(λ))\n",
    "print(\"U inverse:\")\n",
    "print(la.inv(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick check, we print A, then multiply U, D and U inverse into each other and check if that indeed gives us A: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying if A=UDU^(-1):\n",
      "A:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "UDU^(-1):\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "B = (np.dot(U,np.dot(np.diag(λ), la.inv(U)))).real\n",
    "print(\"Verifying if A=UDU^(-1):\")\n",
    "print(\"A:\")\n",
    "print(A)\n",
    "print(\"UDU^(-1):\")\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the correctness of the method. Note that we won't repeat this step for the next phases, as there are errors occuring in the code (which is normal as this is a basic implementation and the QR algorithm is not using shifts) and the multiplication won't be absolutely equal to the original matrix, but very close to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we'll evaluate the outputs and look at some interesting results for some special cases.\n",
    "\n",
    "The special cases will be symmetric, positive definite, diagonal, block, non-singular, and non-square matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Symmetric Matrix Example</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following symmetric matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:3\n",
      "Enter the number of columns:3\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "1 7 3 7 4 -5 3 -5 6\n",
      "A:\n",
      "[[ 1.  7.  3.]\n",
      " [ 7.  4. -5.]\n",
      " [ 3. -5.  6.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) \n",
    "A = np.array(entries).reshape(n, m) \n",
    "print(\"A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the eigen-decomposition gives us the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ:\n",
      "[-7.00792852  7.03594585 10.97198268]\n",
      "U:\n",
      "[[ 0.68415383 -0.62878419 -0.36954564]\n",
      " [-0.61391034 -0.22292725 -0.75724338]\n",
      " [-0.39376087 -0.74493885  0.53853365]]\n",
      "D:\n",
      "[[-7.00792852+0.j  0.        +0.j  0.        +0.j]\n",
      " [ 0.        +0.j  7.03594585+0.j  0.        +0.j]\n",
      " [ 0.        +0.j  0.        +0.j 10.97198268+0.j]]\n",
      "U inverse:\n",
      "[[ 0.68415383 -0.61391034 -0.39376087]\n",
      " [-0.62878419 -0.22292725 -0.74493885]\n",
      " [-0.36954564 -0.75724338  0.53853365]]\n"
     ]
    }
   ],
   "source": [
    "λ,U = la.eig(A)\n",
    "print(\"λ:\")\n",
    "print(λ.real)\n",
    "print(\"U:\")\n",
    "print(U.real)\n",
    "print(\"D:\")\n",
    "print(np.diag(λ))\n",
    "print(\"U inverse:\")\n",
    "print(la.inv(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two notable points here. First, we can see that U inverse = U transpose, which means that during the eigen-decomposition, instead of computing A=U*diag(λ)*U^-1, we can simply compute A= U*diag(λ)*U.T and the result will be the same.\n",
    "\n",
    "The second point is that all the eigenvalues are real numbers, and there are no complex numbers present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Positive Definite Matrix Example</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we present a positive definite matrix below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:2\n",
      "Enter the number of columns:2\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "1 0 0 1\n",
      "A:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) \n",
    "A = np.array(entries).reshape(n, m) \n",
    "print(\"A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigen-decomposition looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ:\n",
      "[1. 1.]\n",
      "U:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "D:\n",
      "[[1.+0.j 0.+0.j]\n",
      " [0.+0.j 1.+0.j]]\n",
      "U inverse:\n",
      "[[ 1. -0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "λ,U = la.eig(A)\n",
    "print(\"λ:\")\n",
    "print(λ.real)\n",
    "print(\"U:\")\n",
    "print(U.real)\n",
    "print(\"D:\")\n",
    "print(np.diag(λ))\n",
    "print(\"U inverse:\")\n",
    "print(la.inv(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that all eigen values are positive, and not a single negative or zero value is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Diagonal Matrix Example</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple diagonal matrix is printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:3\n",
      "Enter the number of columns:3\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "6 0 0 0 7 0 0 0 4\n",
      "A:\n",
      "[[6. 0. 0.]\n",
      " [0. 7. 0.]\n",
      " [0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) \n",
    "A = np.array(entries).reshape(n, m) \n",
    "print(\"A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the eigen-decomposition is printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ:\n",
      "[6. 7. 4.]\n",
      "U:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "D:\n",
      "[[6.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 7.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 4.+0.j]]\n",
      "U inverse:\n",
      "[[ 1.  0. -0.]\n",
      " [ 0.  1. -0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "λ,U = la.eig(A)\n",
    "print(\"λ:\")\n",
    "print(λ.real)\n",
    "print(\"U:\")\n",
    "print(U.real)\n",
    "print(\"D:\")\n",
    "print(np.diag(λ))\n",
    "print(\"U inverse:\")\n",
    "print(la.inv(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point worth considering is that the eigenvalues are equal to the non-zero values present on the diagonal of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Block Matrix Example</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following matrix to be a block matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:5\n",
      "Enter the number of columns:5\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "2 0 0 0 0 0 2 0 0 0 1 1 3 0 0 1 1 0 3 0 1 1 0 0 3\n",
      "A:\n",
      "[[2. 0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0.]\n",
      " [1. 1. 3. 0. 0.]\n",
      " [1. 1. 0. 3. 0.]\n",
      " [1. 1. 0. 0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) \n",
    "A = np.array(entries).reshape(n, m) \n",
    "print(\"A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we assume that A has four blocks, namely B,C,D, and E:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[['B' 'C']\n",
      " ['D' 'E']]\n"
     ]
    }
   ],
   "source": [
    "print('A:')\n",
    "print(np.array([['B','C'],['D','E']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B:\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "C:\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "D:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "E:\n",
      "[[3 0 0]\n",
      " [0 3 0]\n",
      " [0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "print('B:')\n",
    "print(np.array([[2,0],[0,2]]))\n",
    "print('C:')\n",
    "print(np.zeros([2, 3], dtype = int))\n",
    "print('D:')\n",
    "print(np.array([[1,1],[1,1],[1,1]]))\n",
    "print('E:')\n",
    "print(np.array([[3,0,0],[0,3,0],[0,0,3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ:\n",
      "[3. 3. 3. 2. 2.]\n",
      "U:\n",
      "[[ 0.   0.   0.   0.5  0. ]\n",
      " [ 0.   0.   0.   0.   0.5]\n",
      " [ 0.   0.   1.  -0.5 -0.5]\n",
      " [ 1.   0.   0.  -0.5 -0.5]\n",
      " [ 0.   1.   0.  -0.5 -0.5]]\n",
      "D:\n",
      "[[3.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 3.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 3.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 2.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 2.+0.j]]\n",
      "U inverse:\n",
      "[[ 1.  1. -0.  1.  0.]\n",
      " [ 1.  1. -0.  0.  1.]\n",
      " [ 1.  1.  1.  0.  0.]\n",
      " [ 2. -0.  0.  0.  0.]\n",
      " [ 0.  2.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "λ,U = la.eig(A)\n",
    "print(\"λ:\")\n",
    "print(λ.real)\n",
    "print(\"U:\")\n",
    "print(U.real)\n",
    "print(\"D:\")\n",
    "print(np.diag(λ))\n",
    "print(\"U inverse:\")\n",
    "print(la.inv(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting point is that the eigenvalues of A, are equal to the eigenvalues of B and E, combined. B is a diagonal matrix and as seen above, its' eigenvalues are the diagonal elements, therefore [2,2] are the eigenvalues of B. The same applies for E, and the eigenvalues are [3,3,3]. Now looking at the eigenvalues of A, we see that [3,3,3,2,2]=[3,3,3]+[2,2], which proves our point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Non-singular Matrix Example</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example is a non-singular matrix, that is, a matrix which is not invertible(the determinant is 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:2\n",
      "Enter the number of columns:2\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "3 3 5 5\n",
      "A:\n",
      "[[3. 3.]\n",
      " [5. 5.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) \n",
    "A = np.array(entries).reshape(n, m) \n",
    "print(\"A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the eigen-decomposition gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ:\n",
      "[0. 8.]\n",
      "U:\n",
      "[[-0.70710678 -0.51449576]\n",
      " [ 0.70710678 -0.85749293]]\n",
      "D:\n",
      "[[0.+0.j 0.+0.j]\n",
      " [0.+0.j 8.+0.j]]\n",
      "U inverse:\n",
      "[[-0.88388348  0.53033009]\n",
      " [-0.72886899 -0.72886899]]\n"
     ]
    }
   ],
   "source": [
    "λ,U = la.eig(A)\n",
    "print(\"λ:\")\n",
    "print(λ.real)\n",
    "print(\"U:\")\n",
    "print(U.real)\n",
    "print(\"D:\")\n",
    "print(np.diag(λ))\n",
    "print(\"U inverse:\")\n",
    "print(la.inv(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that one of the eigen values is zero, which also indicates that the matrix is non-singular. In singular matrices, each eigen value is non-zero, in conclusion, this matrix has to have a zero eigen value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Non-square Matrix Example</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last example is a non-square matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:2\n",
      "Enter the number of columns:3\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "1 2 3 4 5 6\n",
      "A:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) \n",
    "A = np.array(entries).reshape(n, m) \n",
    "print(\"A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the eigen-decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected square matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cff4b109bd4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mλ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mla\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"λ:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mλ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"U:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meig\u001b[1;34m(a, b, left, right, overwrite_a, overwrite_b, check_finite, homogeneous_eigvals)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected square matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[0moverwrite_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moverwrite_a\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_datacopied\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: expected square matrix"
     ]
    }
   ],
   "source": [
    "λ,U = la.eig(A)\n",
    "print(\"λ:\")\n",
    "print(λ.real)\n",
    "print(\"U:\")\n",
    "print(U.real)\n",
    "print(\"D:\")\n",
    "print(np.diag(λ))\n",
    "print(\"U inverse:\")\n",
    "print(la.inv(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code gave us a value error, as seen. That's because non-square matrices don't have a Spectral decomposition, and computing eigenvalues and vectors is impossible for them. That's why we use the SVD decomposition on these type of matrices instead. The singular values and vectors in non-square matrices is equivalent to the eigenvalues and vectors in square ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 2</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part implements the QR decomposition using all three methods. For simplification, each method is defined in a seperate function, and then they are grouped in a single class named QR_decomposition.\n",
    "\n",
    "The methods are implemented completely based on theoretical concepts of Linear Algebra, but let's have a brief review of how each algorithm computes the decomposition.\n",
    "\n",
    "First, we have the gram schmidt process. In line 5, we initialize an empty orthogonal matrix named Q. In the for loop, we compute Q by computing projections and normalizing them at each iteration and then placing this orthogonal basis as the columns of Q. We then compute the upper-triangular matrix R as (Q.T)(A), and return the (Q,R) tuple as the factorization. So, basically, the core of this method is computing orthogonal factors and normalizing them.\n",
    "\n",
    "Next, we have the Householder reflection in line 23. We start by empty matrices Q,R, and then in the for loop we iterate over the column sub-vectors and compute the householder matrix to zero out the lower triangular matrix entries. So, householder reflections compute the householder orthogonal matrices iteratively, but give the orthogonal factors in the en.\n",
    "\n",
    "The last one is the Givens rotation. Similar to the householder method, we start with empty matrices, and then iterate in a loop, but instead of computing householder matrices, we compute givens rotation matrices to zero-out the lower triangular matrix entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QR_decomposition:\n",
    "    \n",
    "    def gram_schmidt_process(A):\n",
    "        (num_rows, num_cols) = np.shape(A)\n",
    "        Q = np.empty([num_rows, num_rows])\n",
    "        count = 0\n",
    "\n",
    "        for a in A.T:\n",
    "            u = np.copy(a)\n",
    "            for i in range(0, count):\n",
    "                proj = np.dot(np.dot(Q[:, i].T, a), Q[:, i])\n",
    "                u = u - proj\n",
    "\n",
    "            e = u / la.norm(u)\n",
    "            Q[:, count] = e\n",
    "\n",
    "            count += 1  \n",
    "\n",
    "        R = np.dot(Q.T, A)\n",
    "\n",
    "        return (Q, R)    \n",
    "\n",
    "    def householder_reflection(A):\n",
    "        (num_rows, num_cols) = np.shape(A)\n",
    "        Q = np.identity(num_rows)\n",
    "        R = np.copy(A)\n",
    "\n",
    "        for i in range(num_rows - 1):\n",
    "            x = R[i:, i]\n",
    "\n",
    "            e = np.zeros_like(x)\n",
    "            e[0] = copysign(la.norm(x), -A[i, i])\n",
    "            u = x + e\n",
    "            v = u / la.norm(u)\n",
    "\n",
    "            Q_i = np.identity(num_rows)\n",
    "            Q_i[i:, i:] -= 2.0 * np.outer(v, v)\n",
    "\n",
    "            R = np.dot(Q_i, R)\n",
    "            Q = np.dot(Q, Q_i.T)\n",
    "\n",
    "        return (Q, R)\n",
    "\n",
    "\n",
    "    def givens_rotation(A):\n",
    "        (num_rows, num_cols) = np.shape(A)\n",
    "        Q = np.identity(num_rows)\n",
    "        R = np.copy(A)\n",
    "        (rows, cols) = np.tril_indices(num_rows, -1, num_cols)\n",
    "    \n",
    "        for (row, col) in zip(rows, cols):\n",
    "            if R[row, col] != 0:\n",
    "                \n",
    "                r = hypot(R[col, col], R[row, col])\n",
    "                c = R[col, col]/r\n",
    "                s = -(R[row, col])/r\n",
    "            \n",
    "                G = np.identity(num_rows)\n",
    "                G[[col, row], [col, row]] = c\n",
    "                G[row, col] = s\n",
    "                G[col, row] = -s\n",
    "\n",
    "                R = np.dot(G, R)\n",
    "                Q = np.dot(Q, G.T)\n",
    "\n",
    "        return (Q, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core tool of the QR algorithm is now defined, with all three methods, and therefore we can move on to coding the algorithm.\n",
    "\n",
    "The algorithm is fairly simple, it is a function which takes two values as input, an array and a stopping criteria named tol, which is set to 0.000000000001 here. The algorithm computes a QR decomposition of the input array first, using either of the three methods and stores them inside variables Q and R (you can play with these by commenting two of them, and setting one up. As a default, the givens QR decomposition is set here as it gives less numerical errors compared to the later two). Then it bulids an empty matrix of size Q, and names it previous.Previous is used to store the Q matrices from the previous steps, so that it can be used it the next ones. We also make a copy of the Q matrix and store it in a variable named v. \n",
    "\n",
    "Lines 7 to 15 work iteratively for 500 times. Let's explain a single iteration. By now we've decomposed the input array into QR. We form the next matrix by computing RQ. Then, we compute another QR decomposition from this next matrix and repeat the process 500 times. In each iteration we check if the formed matrix is close to the triangular form of the input array using np.allclose() and the given criteria tol. If it is, we break out of the loop and accept the formed matrix as the one converged to the triangular form of the input array, and store it's diagonal values in λ as the eigenvalues of the original matrix. \n",
    "\n",
    "For eigenvectors, we multiply the Q matrices from each step of the iteration into each other using the v variable, and at the final step of the iteration, or when we break out of the loop, we accept v as the matrix containing the eigenvectors of the input array on its columns.\n",
    "\n",
    "Finally we return the tuple (λ,v) as the calculated eigenvalues and eigenvectors of the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QR_algorithm(arr , tol=0.000000000001):\n",
    "    #Q,R = QR_decomposition.gram_schmidt_process(arr)\n",
    "    #Q,R = QR_decomposition.householder_reflection(arr)\n",
    "    Q,R = QR_decomposition.givens_rotation(arr)\n",
    "    previous = np.empty(shape=Q.shape)\n",
    "    v = Q\n",
    "    for i in range(500):\n",
    "        previous[:] = Q\n",
    "        A = R @ Q\n",
    "        #Q,R = QR_decomposition.gram_schmidt_process(A)\n",
    "        #Q,R = QR_decomposition.householder_reflection(A)\n",
    "        Q,R = QR_decomposition.givens_rotation(A)\n",
    "        v = v @ Q\n",
    "        if np.allclose(A,np.triu(A),atol=tol):\n",
    "            break\n",
    "\n",
    "    λ= np.diagonal(A)\n",
    "    return (λ,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the algorithm on an example presented at the beginning of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:3\n",
      "Enter the number of columns:3\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "1 2 3 4 5 6 7 8 9\n",
      "A:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) \n",
    "A = np.array(entries).reshape(n, m) \n",
    "print(\"A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we won't use any packages, and we'll compute the eigen-decomposition with our implementations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ:\n",
      "[ 1.61168440e+01 -1.11684397e+00 -1.00511917e-16]\n",
      "U:\n",
      "[[ 0.23197069  0.88290596 -0.40824829]\n",
      " [ 0.52532209  0.23952042  0.81649658]\n",
      " [ 0.8186735  -0.40386512 -0.40824829]]\n",
      "D:\n",
      "[[ 1.61168440e+01  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.11684397e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.00511917e-16]]\n",
      "U inverse:\n",
      "[[ 0.23197069  0.52532209  0.8186735 ]\n",
      " [ 0.88290596  0.23952042 -0.40386512]\n",
      " [-0.40824829  0.81649658 -0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "λ,U =  QR_algorithm(A)\n",
    "print(\"λ:\")\n",
    "print(λ.real)\n",
    "print(\"U:\")\n",
    "print(U.real)\n",
    "print(\"D:\")\n",
    "print(np.diag(λ))\n",
    "print(\"U inverse:\")\n",
    "print(la.inv(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the two results to each other we observe that the results are fairly close. Thus, the implementation works and we've bulit an eigen-decomposition package from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving into the next part, we'll briefly compare the three methods for computing the QR decomposition. \n",
    "\n",
    "First let's consider the Gram-Schmidt process. For an mxn matrix, the algorithm costs mn^2-n2/3 flops.\n",
    "The most significant advantage of this method is its simplicity, therefore making it easy to implement and use in several places. However, as a result of using several projections, the algorithm is numerically unstable and the results are prone to numerical errors. The problem is that this method computes the orthogonal factors directly, and while there should be a good orthogonality, there isn't as numbers get rounded in each step and we have rounding errors. This may not seem that big of a deal in matrices with a small condition number, but if we consider ones that already have a huge condition number and are unstable naturally, using Gram-Schmidt causes a lot of problems. Therefore, the algorithm works better if we have small, well-conditioned matrices, but is not recommended in bigger dimensions (more than 3) or ill-conditioned ones. \n",
    "\n",
    "The Householder Reflection has the same number of flops as the Gram-Schmidt process, but the differance is that the orthogonal factors are presented at the end, and the rounding errors cause less of a problem as the orthogonal factor is more accurate, therefore it is said that this method is numerically more stable than the Gram-Schmidt process, and is most recommended in ill-conditioned matrices as it also helps with the condition and stability.\n",
    "\n",
    "The givens rotation is also numerically more stable than the Gram-Schmidt process, but in comparison with the Householder reflection, it needs about twice the flops for calculations. Therefore, that can cause a problem in large dense matrices. Generally speaking, if we have large matrices, we should consider its density. If the matrix is very dense and has a large number of non zero elements, it is best to use the Householder reflections. In contrast, if the matrix is sparse and lots of zeroes are present, the Givens rotations work better as they simply skip the zeroes and work on the remaining elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to write the last part, and obtain a complete program by putting the pieces together. Before that we'll focus on our last function named SVD, which takes an array as input and computes the singular value decomposition. \n",
    "\n",
    "The SVD decomposition is also based on theories from linear algebra.As we know, the SVD decomposition is mostly meaningful for non-square matrices that don't have an eigen-decomposition. For square ones, the SVD decomposition is equivalent to the Spectral decomposition.\n",
    "\n",
    "In order to obtain the singular values and borders of a non-square matrix, we compute the matrix multiplications A*A.T and A.T*A, and calculate their eigenvalues and eigenborders. The eigenvalues are the same for both matrices. Lines 2 and 5 of the function do that.\n",
    "\n",
    "Then, as theories state, in order to compute the singular values of the original matrix, all we have to do is sort the eigenvalues of A*A.T or A.T*A, build a diagonal matrix out of them and put the square root of the sorted eigenvalues on the diagonal. The diagonal elements are the singular values of the original matrix. That's what line 8 does. As the eigenvalues are already sorted by the QR algorithm, all it has to do is take the square root and store them in σ.\n",
    "\n",
    "Next, for singular left and right borders, we have to pay attention to the eigenvectors of A.T*A and A*A.T and extract some columns from them. As theories state, the former is done by looking at the indices of the sorted eigenvectors, and extracting the corresponding vectors from the eigenvectors as the left and right singular vectors of the original matrix. Lines 10 to 25 do that. First two empty lists named U and V are created, then by looking at the eigenvalue indices, the representing columns are extracted from the eigenvectors ans stored in the lists. The lists are then converted to numpy arrays.\n",
    "\n",
    "Finally, (U,σ,V.T) is returned as the SVD decomposition of the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD(arr):\n",
    "    λ1,X1 = QR_algorithm(np.dot(arr,arr.T))\n",
    "    λ2,X2 = QR_algorithm(np.dot(arr.T,arr))\n",
    "\n",
    "    σ = (np.sqrt(λ2)).real\n",
    "    eig_count = λ2.shape[0]\n",
    "    U = []\n",
    "    V = []\n",
    "\n",
    "    for i in range(0,eig_count):\n",
    "        U.append(X1[:,i].tolist())\n",
    "\n",
    "    U = np.array(U).T\n",
    "    \n",
    "    for i in range(0,eig_count):\n",
    "        V.append(X2[:,i].tolist())\n",
    "\n",
    "    V = np.array(V).T \n",
    "    return (U,σ,V.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, and for the sake of comparsion, let's consider a non-square matrix like the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:3\n",
      "Enter the number of columns:2\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "1 -1 2 4 -2 1\n",
      "A:\n",
      "[[ 1. -1.]\n",
      " [ 2.  4.]\n",
      " [-2.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) \n",
    "A = np.array(entries).reshape(n, m) \n",
    "print(\"A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll compute the singular values and vectors using the SVD package from scipy, and display the results, then we will repeat the process using our SVD function, and compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values of A:\n",
      "[4.49742282 2.60253491]\n",
      "\n",
      "Left-singular vectors of A:\n",
      "[[ 0.11265596  0.50732793]\n",
      " [-0.99338612  0.07671205]\n",
      " [-0.02219558 -0.85833189]]\n",
      "\n",
      "Right-singular vectors of A:\n",
      "[[-0.40683858 -0.91350006]\n",
      " [ 0.91350006 -0.40683858]]\n"
     ]
    }
   ],
   "source": [
    "U, σ, VT = la.svd(A,full_matrices=False)\n",
    "print(\"Singular values of A:\")\n",
    "print(σ)\n",
    "print(\"\\nLeft-singular vectors of A:\")\n",
    "print(U)\n",
    "print(\"\\nRight-singular vectors of A:\")\n",
    "print(VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values of A:\n",
      "[4.49742282 2.60253491]\n",
      "\n",
      "Left-singular vectors of A:\n",
      "[[ 0.11265596  0.50732793]\n",
      " [-0.99338612  0.07671205]\n",
      " [-0.02219558 -0.85833189]]\n",
      "\n",
      "Right-singular vectors of A:\n",
      "[[ 0.40683858  0.91350006]\n",
      " [-0.91350006  0.40683858]]\n"
     ]
    }
   ],
   "source": [
    "U, σ, VT = SVD(A)\n",
    "print(\"Singular values of A:\")\n",
    "print(σ)\n",
    "print(\"\\nLeft-singular vectors of A:\")\n",
    "print(U)\n",
    "print(\"\\nRight-singular vectors of A:\")\n",
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the results and comparing them gives us a satisfaction over the implemented SVD function, as the values and vectors calculated by both methods are fairly close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, all our efforts can be converted to a complete decomposition program. The program takes a matrix as input and checks whether it's square or not. If it is, the eigen-decomposition is computed and printed (using our implented QR decompositions and algorithm), and if it isn't, it computes the SVD decomposition using our bulit function. \n",
    "\n",
    "Let's take a look at two examples, one square and one non-square:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Square Matrix Example</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:2\n",
      "Enter the number of columns:2\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "9 5 5 18\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[ 9.  5.]\n",
      " [ 5. 18.]]\n",
      "λ:\n",
      "[20.22681202  6.77318798]\n",
      "U:\n",
      "[[ 0.40683858 -0.91350006]\n",
      " [ 0.91350006  0.40683858]]\n",
      "D:\n",
      "[[20.22681202  0.        ]\n",
      " [ 0.          6.77318798]]\n",
      "U inverse:\n",
      "[[ 0.40683858  0.91350006]\n",
      " [-0.91350006  0.40683858]]\n"
     ]
    }
   ],
   "source": [
    "if(n==m): \n",
    "    A = np.array(entries).reshape(n, n) \n",
    "    print(\"A:\")\n",
    "    print(A)\n",
    "\n",
    "    λ,U =  QR_algorithm(A)\n",
    "    print(\"λ:\")\n",
    "    print(λ.real)\n",
    "    print(\"U:\")\n",
    "    print(U.real)\n",
    "    print(\"D:\")\n",
    "    print(np.diag(λ))\n",
    "    print(\"U inverse:\")\n",
    "    print(la.inv(U))\n",
    "\n",
    "else: \n",
    "    A = np.array(entries).reshape(n, m)\n",
    "    print(\"A:\")\n",
    "    print(A)    \n",
    "    U, σ, VT = SVD(A)\n",
    "    print(\"Singular values of A:\")\n",
    "    print(σ)\n",
    "    print(\"\\nLeft-singular vectors of A:\")\n",
    "    print(U)\n",
    "    print(\"\\nRight-singular vectors of A:\")\n",
    "    print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Non-square Matrix Example</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a matrix by filling in the number of rows,columns and values(with real numbers).\n",
      "Enter the number of rows:3\n",
      "Enter the number of columns:2\n",
      "Now, enter the matrix values in a single line separated by space: \n",
      "1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a matrix by filling in the number of rows,columns and values(with real numbers).\")\n",
    "n = int(input(\"Enter the number of rows:\")) \n",
    "m = int(input(\"Enter the number of columns:\"))   \n",
    "print(\"Now, enter the matrix values in a single line separated by space: \") \n",
    "entries = list(map(float, input().split())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "Singular values of A:\n",
      "[2.44948974 0.        ]\n",
      "\n",
      "Left-singular vectors of A:\n",
      "[[ 0.57735027 -0.70710678]\n",
      " [ 0.57735027  0.70710678]\n",
      " [ 0.57735027  0.        ]]\n",
      "\n",
      "Right-singular vectors of A:\n",
      "[[ 0.70710678  0.70710678]\n",
      " [-0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "if(n==m): \n",
    "    A = np.array(entries).reshape(n, n) \n",
    "    print(\"A:\")\n",
    "    print(A)\n",
    "\n",
    "    λ,U =  QR_algorithm(A)\n",
    "    print(\"λ:\")\n",
    "    print(λ.real)\n",
    "    print(\"U:\")\n",
    "    print(U.real)\n",
    "    print(\"D:\")\n",
    "    print(np.diag(λ))\n",
    "    print(\"U inverse:\")\n",
    "    print(la.inv(U))\n",
    "\n",
    "else: \n",
    "    A = np.array(entries).reshape(n, m)\n",
    "    print(\"A:\")\n",
    "    print(A)    \n",
    "    U, σ, VT = SVD(A)\n",
    "    print(\"Singular values of A:\")\n",
    "    print(σ)\n",
    "    print(\"\\nLeft-singular vectors of A:\")\n",
    "    print(U)\n",
    "    print(\"\\nRight-singular vectors of A:\")\n",
    "    print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 4</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last phase, we'll discuss some applications of algorithms presented in phases 2,3 in the field of Data Mining. The apllications are seperated by the order of the subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Eigenvalues and Eigenvectors</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two concepts are of great importance in the field of data science. They are the key concept behind the SVD decompositions and PCA, which is thoroughly explained in the next sections. There are several other applications and we'll point some of them out.\n",
    "\n",
    "Eigenvalues and vectors provide a summary of matrices and datasets. When solving linear differential equations, we can use them to evaluate the rate of change. They are also applicable in maintaining a relationship between two variables.\n",
    "\n",
    "They can also be used to do dimensionality reduction without loosing information. If we have a big data matrix, we can store the k biggen eigenvectors and values and get a low rank approximation of the matrix which is still good enough and easier to work with. \n",
    "\n",
    "The process described above is also useful in treating ill-conditioned matrices, as the approximated matrix has a lower condition number compared to the original matrix and is therefore useful in decreasing the unstabilitiy.\n",
    "\n",
    "Another important application lies in the field of machine learning, precisely the concept of overfitting. Eigenvalues and vectors can decrease the noise in the data by performing regularisation and therefore prevent overfitting. They are also useful in eliminating features with strong correlations.\n",
    "\n",
    "The eigen-decomposition is also the basis of the page-rank algorithm used in the Google Search engine, but we won't get into the details of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. QR Decomposition</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QR Decomposition has several important applications in Data Science and Machine Learning. An obvious one is the eigenvalue problem. But there are several other applications beside that.\n",
    "\n",
    "One of these applications is solving the least squares problem, which is used countless times in machine learning methods such as regression as an error function, where we want to compute the weights which minimize the function.\n",
    "\n",
    "The least squares problem is defined as solving the equation Ax=b. Naturally one would think that the solution can be computed directly by writing x=(A^-1)b, but if we have more observations than variables then the matrix doesn't have an inverse. As we know, one possible solution is to find a solution which minimizes the sum of squared error by taking derivatives and setting it to zero, but another solution is possible using the QR factorization.\n",
    "\n",
    "Let's get back to Ax=b. Using QR decompositions, we can write Ax=QRx=y, and so x=(QR)^-1y=(R^-1)(Q.T)y. So, all we have to do is factorize A with QR, calculate R inverse and Q transpose and place it in the above equation to obtain the least squares solution easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. SVD Decomposition</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD Decomposition has several applications in Data Science, but the most important one might be Dimensionality Reduction by principal component analysis. The PCA is used in data with large numbers of features or dimensions, where we try to identify uncorrelated variables that resemble the data and preserve the information while reducing the dimensions to escape from the curse of dimensionality. In other words we try to capture the maximum variance with less components, or dimensions.\n",
    "\n",
    "Most books claim that PCA and SVD are closely related, and are almost equivalent. If we want to be exact, considering matrix A=UΣV^-1, the right singular vectors in V are the principle directions in PCA and UΣ are the principle components. Therefore, assuming we have m dimensions in a dataset, reducing it to k dimensions is equal to selecting the first k columns from UΣ.\n",
    "\n",
    "In image processing, sometimes we want to compress an image without messing up with its quality. That's when we can use SVD as there are only a few large singular values in SVD. We can keep the values and vectors that are the largest and get a fairly acceptable approximation of the original image, which is more compressed.\n",
    "\n",
    "SVD is also used for solving linear equations. Let's assume we have a large amount of data stored in a matrix like A, and we have some variables stored in a vector like x. We can write the problem as Ax=0, and would like to calculate the values of x given A. As stated, the data is large, and we have more data than variables, so the equation can't be solved. Another option is to minimize the value of Ax, in which case SVD is of help. Suppose we compute A=UΣVT. If Ax=0 has an exact solution, it means that at least one eigenvalue of A is zero, meaning that AA.T has one zero eigenvalue, and that means that one of the singular values is also zero. Therefore, if one element on the diagonak of Σ is zero, then the equation has a solution and the corresponding singular vector from V is the linear solution.\n",
    " \n",
    "If none of the values are zero, it can be proved that the smallest singular value and the corresponding vector is the solution to the least squares fitting (minimizing) problem. \n",
    "\n",
    "There are several other applications, and explaining them all is not possible in a single project. We'll point some applications out, without going into detail.\n",
    "\n",
    "SVD decomposition has countless applications in Recommender Systems, Image Processing Tasks, Natural Language Processing (NLP), Spectral Clustering, and eigenfaces. The list goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recources</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Eldén, Lars. Matrix methods in data mining and pattern recognition. Vol. 15. Siam, 2019.\n",
    "2. https://mathoverflow.net/questions/227543/why-householder-reflection-is-better-than-givens-rotation-in-dense-linear-algebr\n",
    "3. https://math.stackexchange.com/questions/781872/householder-vs-gram-schmidt-orthogonalization-which-should-i-use\n",
    "4. https://en.wikipedia.org/wiki/QR_decomposition\n",
    "5. Advanced Computational Data Mining-class notes\n",
    "6. https://medium.com/fintechexplained/what-are-eigenvalues-and-eigenvectors-a-must-know-concept-for-machine-learning-80d0fd330e47\n",
    "7. https://towardsdatascience.com/qr-matrix-factorization-15bae43a6b2\n",
    "8. https://medium.com/data-science-group-iitr/singular-value-decomposition-elucidated-e97005fb82fa\n",
    "9. https://www.analyticsvidhya.com/blog/2019/08/5-applications-singular-value-decomposition-svd-data-science/\n",
    "10. http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
